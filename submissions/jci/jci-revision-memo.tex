
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[hidelinks,12pt,letterpaper]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
       
% settings:
%\author{Jason Poulos} % Define the authors

%***% change title %***%
%\title{Jason Poulos --- PS290} % Define the title
%\pagestyle{headings} % Define the page style for page numbers and headings
%\addtolength{\textheight}{1cm}
%\makeindex

%% Header
%\usepackage{fancyhdr}
%\thispagestyle{fancy}  %change to pagestyle for fancy on all pages
%\fancyhf{} % delete current setting for header and footer
%\fancyhead[L]{\textbf{Ms. No. XPS-D-16-00013}}
%\fancyhead[C]{}
%\fancyhead[R]{Revision Memo}
%\renewcommand{\headrulewidth}{0.5pt}
%\renewcommand{\footrulewidth}{0pt}
%\addtolength{\headheight}{3pt} % make space for the rule
%\fancypagestyle{plain}{%
%       \fancyhead{} % get rid of headers on plain pages
%       \renewcommand{\headrulewidth}{0pt} % and the line
%}

% Subsubsection
\usepackage{titlesec}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% Margins
\usepackage[margin=1in]{geometry} %1 inch margins

%Spacing
%\usepackage{setspace}
%\doublespacing

%indent and break
\usepackage{parskip}
\setlength{\parindent}{15pt}

% Appendix
\usepackage[page]{appendix}
\usepackage{graphicx}
\usepackage{longtable}

% Etc
\usepackage{hyperref}
\usepackage{amsmath}

%New commands
\newcommand{\possessivecite}[1]{\citeauthor{#1}'s [\citeyear{#1}]} 

% Reference labels
\usepackage{xr}
\externaldocument{patt-noncompliance}

% Table of contents
\renewcommand*\contentsname{Revision Memo:\\ ``Estimating population average treatment effects from experiments with noncompliance"\\ (DGJCI.2018.0011)}

% Chicago 15 ed. author-date
\usepackage[utf8]{inputenc}
%\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[authordate,backend=biber,natbib]{biblatex-chicago}
\addbibresource{refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} 

\tableofcontents

\pagebreak

\section{Reviewer: 1 (R1)}

\subsection{Expand set of simulations}

%  I am confused as to why, in Figure 3, the Sample Average Treatment Effect has so much better RMSE results when the compliance rate is low rather than when it is high compared with the population average treatment effect.  I guess the issue is that bias is coming from two directions: failure of the CACE to be the same marginally in the RCT and the population, and failure of the compliance distribution to be the same in the RCT and population, so that the former is constant in the simulation and thus the SATT does not varies as compliance changes? 

%Perhaps the solution here is to expand the set of simulations to be more of an experimental design themselves, by varying the parameters that determine the degree of confounding with sample selection, confounding with the treatment assignment, and confounding with compliance.  Your current design can handle this I believe – you just need to expand the set of parameters considered in your models on p. 8.  Otherwise the simulation results would suggest to me that I shouldn’t use the PATT approach unless my compliance rate is high, which is presumably the opposite of what the authors are trying to suggest.

Following R1's excellent suggestion, we expand the set of simulations to be more of an experimental design themselves, by varying the parameters that determine the degree of confounding with sample selection, confounding with treatment assignment, and confounding with compliance. In addition, we made the following changes to the simulations:

\begin{itemize}
	\item Compare the PATT and complier-adjusted PATT (PATT-C) estimators against the SATE, which is just the ITT effect scaled by the compliance rate, since the SATE is a more commonly used estimator
	\item Use gradient boosting to predict compliance rate in the RCT and to predict the potential outcomes of observed and predicted RCT compliers
	\item Vary the $e_k$, $k = \{1...6\}$ parameters along a grid of five standard normal deviates
\end{itemize}

In the new simulations, we find that the estimation error of PATT-C is invariant to increases in the compliance rate and also increases in the three dimensions of confounding. In comparison, SATE performs worse when the compliance rate is low, and is also considerably more variable than both of the population estimators due to the fact that SATE is unable to account for differences in pretreatment covariates between the RCT sample and target population.

\subsection{Presentation}

\subsubsection{Assumptions}
%First, I would encourage that the assumptions 1-4 be restated in the text rather than in the appendix (hopefully the Editors will allow this even if there are length constraints). 

Following R1's suggestion, we have moved Assumptions \ref{consistency} -- \ref{sutva} from the Appendix into the main text. 

\subsubsection{Consistency for index $i$}
%Second, the subject level index i is inconsistently used – either use it everywhere, nowhere, or be clear when it is dropped. '

R1 points out that the subject-level index $i$ is inconsistently used in the original manuscript. The revised manuscript ensures that all subject-level quantities are indexed with $i$. 

\subsubsection{Define SATT}
%Third, the SATT is introduced on p. 7 but not defined that I could see. 
In the revised manuscript, we compare the PATT and PATT-C estimators against the SATE, which is just the ITT effect scaled by the compliance rate, since the SATE is a more common estimator. 
 
\subsubsection{NHIS sample design}

%Fourth, I am guessing that the complex sample design of the NHIS was ignored in the application.  I wouldn’t require that this be accounted for, but this should at least be noted as a limitation. 
R1 points out that the complex sample design of the NHIS was ignored in the application. Footnote \ref{nhis-ftn} notes this as a limitation and also references a National Center for Health Statistics report on the NHIS sampling design. The revised manuscript also properly cites NHIS as a data source. 

\subsubsection{Interpretation of results}
%Fifth, I was surprised at the number of interactions there were in the Medicaid expansion results – I am not arguing with the results, but it might be nice to see if this is a reasonable finding, either by consulting the broader literature or health care policy experts in this area.  

R1 suggests providing more context for interpreting whether the subgroup analyses presented in Section \ref{results} are reasonable. We expand Section \ref{results} in the revised manuscript to compare our sample subgroup estimates with those published in the online appendices of \citep{Taubman} and \citet{NBERw22363}. These studies do not perform subgroup analyses for the broader population. Similar to our sample estimates, these studies find considerable treatment effect heterogeneity in terms of gender, age, smoking status, and pre-lottery welfare participation. 

%Finally, it seems that the results in Figures 4 and 5 contradict the wording on the next to last paragraph of the paper (p. 19) – the figures suggest that the PATT estimates suggest an increase in ER visits and a decrease in outpatient visits, whereas the text states the opposite.

In addition, R1 points out that in the Discussion (Section \ref{discussion}) of the original manuscript, the direction of the PATT estimates on ER and outpatient visits are incorrectly described. The revision removes this description as it is not relevant to the discussion on subgroup analyses. 

\section{Reviewer: 2 (R2)}

\subsection{Identifying assumptions}

\subsubsection{Notation}

\paragraph{Conditioning on $D_i$}
%On line 38 of page 3, the authors reference Assumptions (1) - (4), which are made in Hartman et al. (2015). In the appendix, these assumptions frame the ignorability assumptions conditional on T = 1 and C = 1. I’m curious why these assumptions are not made on D = 1 (which is, I assume, the only thing we can observe in the population), and then later combined with the no-defiers assumption. This would be more consistent with the Hartman et al. (2015) framing, which discusses the need for treatment to mean the same thing in both the experiment and population–so if PATT is defined by receipt of treatment, their counter part would be those who receive treatment in the experiment.

R2 suggests framing the ignorability Assumptions \ref{si_treat} and \ref{si_ctrl} on $D_i$ rather than $T_i$ and $C_i$ to be more consistent with the framing of \citet{Hartman} and to ensure that treatment means the same thing in both the experiment and the population. The reason we decided to frame the ignorability assumptions conditional on $T_i$ and $C_i$ is to distinguish between noncompliers who should have received treatment (i.e., individuals with $T_i=1$ and $D_i = 0$) from noncompliers assigned to control; i.e., individuals with $T_i = 0$ and $D_i = 0$. Conditioning on $T_i$ and $C_i$ is important for deriving the estimator for $\tau_{\text{PATT}}$ (Eq. \ref{tpatt-est}). 

We have made more clear our motivation for conditioning on  $T_i$ and $C_i$ when introducing notation in Section \ref{assumptions} in the revised manuscript. 

\paragraph{$Y$ subscripts}
%The consistency under parallel assumption–in Hartman et al. (2015), this is defined on Yist, but in this case I’m curious if it would be more appropriate to define it based on d? If the compliance patterns/decision might differ in the RCT vs. the population, the potential outcomes might differ based on treatment assign- ment, but I could see an argument that they shouldn’t differ based on receipt of treatment. If this is incorrect, I’d like to see the authors discuss why compliance status isn’t a factor in this assumption.

R2 is correct that it would be more appropriate to define $Y$ based on $d$ rather than $t$. As described in the comment \ref{interpret} below, the original manuscript confused treatment eligibility $T_i$ with treatment received $D_i$. Our estimation procedure relies on fitting a response curve to $D_i$ in the RCT, since we cannot actually observe $T_i$ in the population. We have revised the manuscript accordingly. 

\paragraph{$W_i$} \label{Wi}
%The authors use the same W for all of their assumptions. It seems to be that the set of covariates that predict compliance could be different than those that predict sample selection, and the sets necessary for identification in those steps could be (possibly very) different. It seems that notational ease might be the reason to use the same W across all assumptions, but I’d like to see the authors at least discuss how these processes relate, and how we’d consider the different sets.

With regards to our use of the same $W_i$ across all assumptions, here we are implicitly assuming that the covariates that determine sample selection also determine population treatment assignment and complier status. This is useful not only for notational ease but also reflects our modeling assumption that $W_i$ is the same. In the revised manuscript, we discuss this choice in Sections \ref{assumptions} and \ref{modeling-assumptions}. 

Moreover, in Section \label{ensemble} of the revised manuscript we describe the need for using candidate learners with built-in variable selection methods, such as the Lasso, in the compliance and response model ensembles. The idea is that we input the same $W_i$ and each candidate learner selects the most important covariates for predicting complier status or potential outcomes. 

\subsubsection{Assumption \ref{compl}}

%Assumption 5 conditions on W, but my understanding is that they use the propensity to comply in their estimation strategy. Would it be better to use p(W), or is there a reason that the authors prefer not to use the propensity? It might at least be worth discussing the weaker assumption, and its usefulness in estimation.

%In their proof, the authors don’t make clear where they use Assumption (5). I believe its in the last line, although the authors only reference randomization. It should be in here, I believe, because it is what allows us to use the "complier controls".

R2 is correct that while Assumption \ref{compl} conditions on $W$, we use propensity to comply in the estimation.  In \ref{compliance-model} of the estimation procedure it becomes clear why we write the assumption this way. Assumption \ref{compl} implies that $P(C_i=1 | S_i=1, T_i=1, W_i) = P(C_i=1 | S_i=1, T_i=0, W_i)$. We estimate the first term to get to the second term. In addition, it would be confusing to have a propensity score here because there are different propensities: propensity to comply given $W_i$, propensity to be included in the RCT, propensity to receive treatment in the observational sample. 

We've included in the revised manuscript when stating Assumption \ref{compl} a justification for conditioning on $W$ and it's usefulness in the estimation strategy.

R2 is right in pointing out that the original manuscript doesn't make clear where the assumption is used in the proof. The revised manuscript makes clear -- as suggested by R2 -- that the last line of the proof follows because \ref{compliance-model} allows us to use ``complier controls''. 

\subsubsection{Placebo test}
%In Hartman et al. (2015) there is discussion of a placebo test that can be conducted to provide evidence supporting the identifying assumptions. The test doesn’t go thorough here because the complier treatment and complier controls aren’t exchangeable by design, since we need to assume we know the complier model. Stuart et al (2011) also have a placebo test in their identification stragegy. These tests are incredibly valuable for helping evaluate the identification assumptions. Have the authors considered finding ways to conduct placebo tests in this design? If it is possible, I think it would strengthen the argument quite a bit. The test isn’t straightforward to me, but the authors are more immersed in this identification strategy and might be able to come up with something clever.

R2 helpfully suggests that we conduct placebo tests to provide evidence supporting the identifying assumptions. R2 is correct that a placebo test for Assumption \ref{compl} is not possible because we never observe whether RCT controls would actually take-up treatment if assigned.

However, we are able to compare the observed responses of RCT compliers and the responses of population ``compliers,'' adjusted by the covariate distribution of RCT compliers. Significant difference between the mean outcomes of these groups would indicate that either Assumption \ref{consistency} (for $d=1$), or Assumptions \ref{si_treat} and \ref{si_ctrl} are violated. Section \ref{placebo-tests} of the revised manuscript further describes the placebo tests. 

\subsubsection{Violation of no defiers assumption}

% In the application, I like the authors discussion of the assumptions. This is a great example of how researchers should approach these problems. However, the authors note that the defiers assumption is violated–and they do not discuss how this could impact our findings. I’d like to see more discussion of how to consider violations in this framework. Are there sensitivity analyses we can run? Are there principled ways to think about the impact?

R2 justifiably asks for more discussion regarding how the violation of Assumption \ref{monotonicity} would impact our empirical findings. Section \label{sens-defiers} discusses two sources of bias arising from the presence of defiers: the proportion of defiers in the study and the difference in the average causal effects of treatment received for compliers and defiers. 

In the OHIE, the proportion of defiers is relatively small. We argue that the difference in average causal effects of treatment received for compliers and defiers would have to be considerably large in order for the bias to meaningfully alter the interpretation of the empirical results. 

\subsection{Prediction threshold and modeling assumptions}\label{prediction-threshold}

%In estimation–do the authors just include those individuals who have p>0.5 of complying? Is there some reason to consider a different cutoff? It isn’t clear in the manuscript, so some discussion would be nice. I could see an argument for doing some sort of (propensity, probably) matching.

%I’d like to see the authors note the need for additional modeling assumptions that are necessary for estimation above the identification assumptions.
We use a standard prediction threshold of 50\% to classify compliers, so that complier status $C_i$ is a binary variable. This is necessary for \ref{response-model} in the estimation procedure, where we subset to observed compliers assigned to treatment and predicted compliers assigned to control. 

We describe the prediction threshold and discuss additional modeling assumptions in Section \ref{modeling-assumptions} of the revised manuscript. 

\subsection{Interpretation of estimator}\label{interpret}

%In the simulations, the authors refer to Hartman et al.’s (2015) estimator when discussing the unadjusted PATT estimator. I feel this is an inappropriate comparison–in the face of non-compliance the assumptions in Hartman et al. (2015) wouldn’t be met–thus the need for this extension. I’m not sure exactly how to interpret what this estimator is estimating–the ITT in the experiment extrapolated to those who receive treatment in the population? Or is it the ITT effect in the population–which would imply weighting to the individuals who were eligible for treatment in the population? But, if noncompliance is an issue, wouldn’t assumption (1) be violated for the non-complying individuals if we weighted the ITT to PATT? I’d like a little more discussion on how to think about the interpretation of this estimator.

R2 raises concerns with regards to the interpretation of our proposed estimator (PATT-C). The correct interpretation for the estimator is the following: it's the complier-average causal effect estimated on population, adjusted to what we would have seen in the RCT if treatment received were the same. It is not the ITT extrapolated to those who receive treatment in the population. The revised manuscript clarifies this interpretation when defining PATT-C. 

We believe that R2 is referring to the \citet{Hartman} estimator and wondering how to interpret this estimator \emph{if} there is in fact non-compliance. In this case, R2 is correct in stating that the \citet{Hartman} estimator is the ITT effect extrapolated those who take treatment in the population. 

Confusion regarding the interpretation of our estimator likely results from the original manuscript mixing up treatment eligibility and treatment taken. We have corrected the manuscript and results such that the response curve is fitted to treatment received $D_i$ in the RCT rather than treatment eligibility, $T_i$, since we cannot actually observe $T_i$ in the population. 

\subsection{Relationship to AIR approach}\label{reweighting}
%In estimation of ATE from the ITT in a randomized trial, we usually divide by the compliance rate under the assumptions outlined in Angrist, Imbens, and Rubin (1996) (particularly no defiers), but we aren’t required to actually identify individuals in the control group are actually compliers. The authors instead suggest actually identifying the likely compliers in the control group–a very different strategy that makes sense in the context of the reweighting methods to get from the sample to population effect. I am, first, curious if there is any way to reweight this effect in some way, rather than actually identify the compliers. Second, if not, I think it is worth discussing the relation to this approach, and why we need a different approach. Being able to identify compliers is, of course, a strong assumption, so it might be nice to consider if there were a way to leverage the typical complier effect in the experiment.

R2 points out that when estimating the average treatment effect on treated compliers in a randomized trial, we usually divide the ITT effect by the compliance rate under the assumptions outlined in Angrist, Imbens, and Rubin (1996), and in this approach we don't need to identify individuals in the control group who are compliers.

We include a discussion of the need to identify compliers --- rather than weight the unadjusted PATT estimate by the population compliance rate --- in Section \ref{intro} of the revised manuscript. We decided to take this approach because the compliance rate is likely to differ between the sample and population, as well as across subgroups. Moreover, our approach allows us to to decompose PATT estimates by covariate group. 

\section{Reviewer: 3 (R3)}

\subsection{Generalizability and originality}
%compliance and generalizability aspects of the paper were not sufficiently connected to constitute a strong original contribution. Generalizability hinges on the response surface being the same within the RCT to that in the general population. This is such a strong condition that it detracts from the paper.

%My first reaction was that the solution to the problem posed by the reviewers has been been solved in various ways by Baker and Lindeman (1995), Frangakis and Rubin (1999), O’Malley and Normand (2004) and various other authors. It is only later that the subtlety involving generalization of the RCT result to the general population is revealed. But then the authors don’t do anything special in the generalizability space, which from a pure methodological standpoint makes this a distracting element as nothing beyond the regular (re-weighting) way of generalizing results is proposed.

R3 expressed concern that the compliance and generalizability aspects of the paper are not sufficiently connected to warrant an original contribution. We make two original contributions to the literature on extrapolating RCT results to populations: first, we define the assumptions necessary to identify complier-average causal effects for the target population. The need for this contribution is acknowledged in the discussion of \citet{Hartman}. 

Second, we propose a procedure to estimate this quantity with few additional modeling assumptions. Our estimation procedure is novel in that we are estimating the response surface for RCT compliers and using the predicted values of the response surface model to estimate the potential outcomes of population members who received treatment. We describe in Section \ref{intro} of the revision how this approach differs from reweighting methods that use propensity scores to adjust the RCT data \citep[e.g.,][]{stuart2011use}. Our estimation strategy is also novel in that we are actually predicting which of the RCT controls would have complied to treatment had they been treated. We argue in the revised Section \ref{intro} that this estimation step is important because the compliance rate is likely to differ between the sample and population, as well as across pretreatment covariate group. 


\subsection{Abstract}

%The title and the abstract appear to be in conflict over what is the target of inference due to the fact that generalizing the RCT inferences to the whole population is not mentioned in the Abstract. The last part of the Abstract should be re-written to be more specific about the ultimate inferential goal.

R3 points out that it the ultimate inferential goal of the paper -- i.e., being able to extrapolate RCT sample estimates to a broader population of interest --- is unclear in the abstract. We have rewritten the abstract to make it more clear that we are interested in population-level treatment effect estimates. 

\subsection{DAG}

%From the internal validity standpoint, the role of W in the DAG in Figure 1 appears to be key. If any elements of W are not controlled, then there is a backdoor pathway from T (the IV) back to W and into Y! This point might be further emphasized.

In Section \ref{assumptions} of the revised manuscript, we underscore the importance of $W_i$ in the DAG, as suggested by R3.

%Related to 3), could there be a role for variables that are causes of S but have no direct effect on T? I raise this as a way of making the generalizability aspect more nuanced than simply relying on the response surface and distribution on compliers (conditional on covariates) being the same in the RCT in the population.

R3 asks whether there could there be a role for variables that are causes of $S_i$ but have no direct effect on $T_i$. This question is similar to R2's question about whether $W_i$ is the same across all identifying assumptions (\ref{Wi}). We have decided to keep $W_i$ the same both for ease of exposition and because we use the same covariate sets in our estimation procedure --- a modeling assumption described in Section \ref{estimation} of the revised manuscript. 

\subsection{Clarification re: proof}

%The Proof of Theorem 1 is made difficult to follow because the “by (k)” comments do not relate back to earlier equations. The 4th line of the first expectation derivation does not seem to me to be implied by Equation (2) above. Likewise, there is no Equation (3) to which the third equation of the second expectation pertains. Are the numbers the numbered assumptions?

The ``by (k)'' comments in the Proof of Theorem 1 are supposed to be numbered assumptions. We have corrected the comments in the Proof accordingly. 

%Add the intuitive explanation that conditioning on W makes sample selection ignorable, under the given assumptions. This is the critical connector between lines 3 and 4 of the first expectation derivation.

Following the suggestion of R3, we have added to the discussion of the Proof the intuitive explanation that conditioning on $W_i$ makes sample selection ignorable under Assumption \ref{si_treat}. 

\subsection{Plausibility of strong ignorability assumptions}
%The second last statement of Section 2.3 is a key statement! It’s a strong condition. Therefore, some comment on its plausibility would be helpful. Make it clear why you don’t have a companion assumption for non-compliers.

We've revised Section \ref{assumptions} to include more discussion of potential violations of the strong ignorability assumptions, and Section \ref{verifying} discusses whether these assumptions are plausible for our empirical application. Footnote \ref{ignorability-noncompliers} makes clear why we don’t have companion assumptions for non-compliers.

\subsection{Clarification re: estimation procedure}

\subsubsection{Prediction threshold}
%It is not clear from the text how step 2 of the algorithm in Section 2.3 is operationalized. What is the threshold over which a complier is said to be a “predicted complier”?

Similar to R2's comment (\ref{prediction-threshold}), R3 asks for clarification on which prediction threshold we use to classify compliers. The description of the estimation procedure in Section \ref{estimation} of the revised manuscript clarifies this point. 

\subsubsection{Relationship to reweighting methods}
%Section 3.2 ends with a claim that a re-weighting approach is needed. I don’t think that re-weighting has been discussed up until this point. For completeness, I think that the paper needs to explain the re-weighting process.

The original manuscript states that the simulation results shows that a reweighting approach is needed to extrapolate RCT results to a population. This is confusing, because our estimation strategy does not use a reweighting approach, such as assigning individuals in the RCT and population a weight according the inverse propensity score \citep{stuart2011use}. It instead estimates the response surface for RCT compliers and extrapolates the response surface to treated individuals in the population. Each method accomplishes the same goal of adjusting the RCT data to a population, either by using inverse propensity score weights or the predicted values from a response surface model. 

We differentiate reweighting methods and response surface approach in Section \ref{intro} and also generalize the statement at the end of Section \ref{sim-results} of the revised manuscript to include both reweighting and response surface methods. 

\subsubsection{Description of predictive algorithms}
%Continuing the above point, Appendices 2 and 3 contain results for a number of predictive algorithms. I found it strange that these were not described in the main text at all. I recommend that they are introduced in the main text as several of these methods are not yet household names to all statisticians.

Following R3's suggests, we introduce Section \ref{ensemble} in the revised manuscript that introduces the predictive algorithms and the ensemble method. Additionally, we describe the method of evaluating the predictive accuracy of the ensemble. 

\subsection{Grammatical}

We thank R3 for noticing grammatical errors and we have corrected them in the revised manuscript. 


%Bibliography
\printbibliography


\end{document}